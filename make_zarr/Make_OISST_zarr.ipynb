{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "from glob import glob\n",
    "from os import path\n",
    "import os\n",
    "import numpy as np\n",
    "import fsspec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create NOAA OISST zarr data files\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "adir = 'F:/data/sat_data/sst/noaa_oisst/www.ncei.noaa.gov/data/sea-surface-temperature-optimum-interpolation/v2.1/access/'\n",
    "dir_pattern_zarr = adir + 'avhrr_zarr2/'\n",
    "dir_pattern = adir + 'avhrr/'\n",
    "pattern = dir_pattern + '/*/*.nc'\n",
    "files = glob(pattern)\n",
    "print('number of files:',len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#open dataset, this will take a while\n",
    "ds=xr.open_mfdataset(files,combine='nested',concat_dim='time',decode_cf=False,mask_and_scale=False)\n",
    "ds.close()\n",
    "ds_all = ds.isel(zlev=0)\n",
    "ds_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove any duplicates\n",
    "_, index = np.unique(ds_all['time'], return_index=True)\n",
    "ds_all=ds_all.isel(time=index)\n",
    "#rechunck data  #data in int16 = 2 bytes \n",
    "ds_all = ds_all.chunk({'time':1000,'lat':300,'lon':300})\n",
    "ds_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output data to zarr format\n",
    "#ds_all.to_zarr(dir_pattern_zarr, consolidated=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# append new data to original data store without overwritting entire store\n",
    "\n",
    "- there is a trick, I need to raise and issue on github to document because the documentation isn't clear about:\n",
    "1. that you are appending, so you need to make sure you are only appending the new bit\n",
    "2. that you need to decode your data\n",
    "\n",
    "I struggled to get this to work at first because I tried just reading the data the same way & then appending it. \n",
    "This did not work, I got errors about removing scale_factor, add_offset, Fill_value which are all encoding attributes. After poking around, the error suggest you just remove these attributes, and that is true, if you remove them, it will work, but then the appended data isn't encoded correctly and it is a mess.  so, actually, you have to apply the decoding, then everything is peachy. \n",
    "\n",
    "## First calculate where the old data ends, then read in new data, decode, append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_old = xr.open_zarr(dir_pattern_zarr, consolidated=True,decode_cf=False)\n",
    "ds_old.close()\n",
    "lasttime=ds_old.time[-1] + 1#+np.timedelta64(1, 'D')\n",
    "print(lasttime.data)\n",
    "ds_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds=xr.open_mfdataset(files[-100:],combine='nested',concat_dim='time',decode_cf=False,mask_and_scale=False)\n",
    "ds.close()\n",
    "ds_all = ds.isel(zlev=0)\n",
    "#remove any duplicates\n",
    "_, index = np.unique(ds_all['time'], return_index=True)\n",
    "ds_all=ds_all.isel(time=index)\n",
    "#rechunck data  #data in int16 = 2 bytes \n",
    "ds_all = ds_all.chunk({'time':1000,'lat':300,'lon':300})\n",
    "ds_new = ds_all.sel(time=slice(lasttime,9999999))\n",
    "print(ds_new.time[0:3].data)\n",
    "ds_new = xr.decode_cf(ds_new)\n",
    "ds_new.to_zarr(dir_pattern_zarr, mode='a',append_dim='time', consolidated=True) #\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# slightly different version for AWS Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ikey = fsspec.get_mapper('s3://noaa-oisstv02r2/', anon=False)\n",
    "ds_old = xr.open_zarr(ikey,consolidated=True, decode_cf=False)\n",
    "ds_old.close()\n",
    "lasttime=ds_old.time[-1] + 1#+np.timedelta64(1, 'D')\n",
    "print(lasttime.data)\n",
    "ds_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xr.decode_cf(ds_old.time[-1],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "import xarray as xr\n",
    "\n",
    "s3 = s3fs.S3FileSystem(anon=False)\n",
    "# This generates a list of strings with filenames\n",
    "s3path = 's3://noaa-oisstv02r1/*/*.nc'\n",
    "remote_files = s3.glob(s3path)\n",
    "\n",
    "# Iterate through remote_files to create a fileset\n",
    "fileset = [s3.open(file) for file in remote_files]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds=xr.open_mfdataset(fileset,combine='nested',concat_dim='time',decode_cf=False,mask_and_scale=False)\n",
    "ds.close()\n",
    "ds_all = ds.isel(zlev=0)\n",
    "ds_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove any duplicates\n",
    "_, index = np.unique(ds_all['time'], return_index=True)\n",
    "ds_all=ds_all.isel(time=index)\n",
    "#rechunck data  #data in int16 = 2 bytes \n",
    "ds_all = ds_all.chunk({'time':1000,'lat':300,'lon':300})\n",
    "ds_new = ds_all.sel(time=slice(lasttime,9999999))\n",
    "print(ds_new.time[0:3].data)\n",
    "ds_new = xr.decode_cf(ds_new)\n",
    "ikey = fsspec.get_mapper('s3://noaa-oisstv02r2/', anon=False)\n",
    "ds_new.to_zarr(ikey, mode='a',append_dim='time', consolidated=True) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
