{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example collocating a .cvs dataset (krill) with Pangeo and AWS datasets\n",
    "\n",
    "* Import libraries\n",
    "* set krill filename\n",
    "* define function to parse time in csv file\n",
    "* define function to get cloud data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "# filter some warning messages\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "import xarray as xr\n",
    "import fsspec\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import intake\n",
    "import dask\n",
    "import pandas as pd\n",
    "\n",
    "xr.set_options(display_style=\"html\")  #display dataset nicely \n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = 12, 6\n",
    "%config InlineBackend.figure_format = 'retina' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "warnings.simplefilter('ignore') # filter some warning messages\n",
    "xr.set_options(display_style=\"html\")  #display dataset nicely \n",
    "\n",
    "krill_file = 'F:/data/NASA_biophysical/collocated_data/CorrectMasterKrill.csv'\n",
    "\n",
    "def parse_time(tem):\n",
    "    # subroutine to parse the time in the csv file\n",
    "    i1 = tem.find('/')\n",
    "    smon = str(tem[0:i1]).zfill(2)\n",
    "    tem = tem[i1+1:]\n",
    "    i1 = tem.find('/')\n",
    "    sdym = str(tem[0:i1]).zfill(2)\n",
    "    tem = tem[i1+1:]\n",
    "    i1 = tem.find(':')\n",
    "    syr = str(2000+int(tem[0:i1-2])).zfill(4)\n",
    "    tem = tem[i1-2:]\n",
    "    i1 = tem.find(':')\n",
    "    shr = str(int(tem[:i1])).zfill(2)\n",
    "    tem = tem[i1+1:]\n",
    "    smin = str(tem[0:2]).zfill(2)\n",
    "    tstr = syr+'-'+smon+'-'+sdym+'T'+shr+':'+smin\n",
    "    return tstr\n",
    "\n",
    "#define function to get all the data at once, use same years for climatology for all data\n",
    "def get_data():\n",
    "    \n",
    "    #climatology years\n",
    "    cyr1,cyr2='1993-01-01','2018-12-31'\n",
    "    \n",
    "    # AVISO test\n",
    "    cat_pangeo = intake.open_catalog(\"https://raw.githubusercontent.com/pangeo-data/pangeo-datastore/master/intake-catalogs/master.yaml\")\n",
    "    ds = cat_pangeo.ocean.sea_surface_height.to_dask()\n",
    "    ds = ds.rename({'latitude':'lat','longitude':'lon'})\n",
    "    ds.coords['lon'] = (ds.coords['lon'] + 180) % 360 - 180\n",
    "    ds_aviso = ds.sortby(ds.lon).drop({'lat_bnds','lon_bnds','crs','err'})\n",
    "    for var in ds_aviso:\n",
    "        tem = ds_aviso[var].attrs\n",
    "        tem['var_name']='aviso_'+str(var)\n",
    "        ds_aviso[var].attrs=tem\n",
    "    ds_aviso_clim = ds_aviso.sel(time=slice(cyr1,cyr2))\n",
    "    ds_aviso_clim = ds_aviso_clim.groupby('time.dayofyear').mean('time',keep_attrs=True,skipna=False)    \n",
    "\n",
    "    #sst\n",
    "    file_location = 's3://mur-sst/zarr'\n",
    "    ds_sst = xr.open_zarr(fsspec.get_mapper(file_location, anon=True),consolidated=True)\n",
    "    ds_sst = ds_sst.drop({'analysis_error','mask','sea_ice_fraction'})\n",
    "    tem = ds_sst.analysed_sst.attrs\n",
    "    tem['var_name']='mur_sst'\n",
    "    ds_sst.analysed_sst.attrs=tem\n",
    "    ds_sst_clim = ds_sst.sel(time=slice(cyr1,cyr2))\n",
    "    ds_sst_clim = ds_sst_clim.groupby('time.dayofyear').mean('time',keep_attrs=True,skipna=False)\n",
    "    \n",
    "    #put data into a dictionary\n",
    "    data_dict={'aviso':ds_aviso,\n",
    "               'sst':ds_sst}\n",
    "    clim_dict={'aviso_clim':ds_aviso_clim,\n",
    "               'sst_clim':ds_sst_clim}\n",
    "  \n",
    "    return data_dict,clim_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in krill dataset\n",
    "- use pandas read_csv, this has a lot of built in options that are worth exploring\n",
    "- the NET_IN_TIME has the time, but for reading via a computer, it is a bit of a difficult format because the month and day of month can be either one digit or two digits, so here we parse the data by '/' and ':'\n",
    "- next the lat and lon are scaled by 100, probably to add significant digit accuracy in the cvs file, so we un-scale the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRUISE</th>\n",
       "      <th>HAUL_NO</th>\n",
       "      <th>NET_IN_TIME</th>\n",
       "      <th>NET_IN_LAT</th>\n",
       "      <th>NET_IN_LON</th>\n",
       "      <th>BOTTOM_DEPTH</th>\n",
       "      <th>JRES_STRATA</th>\n",
       "      <th>STATION</th>\n",
       "      <th>T..SPINIFERA</th>\n",
       "      <th>E..PACIFICA</th>\n",
       "      <th>...</th>\n",
       "      <th>Length</th>\n",
       "      <th>Width</th>\n",
       "      <th>Ocean</th>\n",
       "      <th>Canyon_ID</th>\n",
       "      <th>Distance_to_Canyon</th>\n",
       "      <th>LTYPE</th>\n",
       "      <th>INCREM</th>\n",
       "      <th>DEPTH</th>\n",
       "      <th>Distance_to_200_Isobath</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>205</td>\n",
       "      <td>2</td>\n",
       "      <td>5/10/02 2:47</td>\n",
       "      <td>36.463799</td>\n",
       "      <td>121.528496</td>\n",
       "      <td>172</td>\n",
       "      <td>C</td>\n",
       "      <td>114</td>\n",
       "      <td>1965</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>205.637501</td>\n",
       "      <td>59.540925</td>\n",
       "      <td>North Pacific Ocean</td>\n",
       "      <td>C9435</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>isobath</td>\n",
       "      <td>2000</td>\n",
       "      <td>-2000</td>\n",
       "      <td>17917.300130</td>\n",
       "      <td>2002-05-10 02:47:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>205</td>\n",
       "      <td>3</td>\n",
       "      <td>5/10/02 3:56</td>\n",
       "      <td>36.423999</td>\n",
       "      <td>121.544102</td>\n",
       "      <td>86</td>\n",
       "      <td>C</td>\n",
       "      <td>115</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>205.637501</td>\n",
       "      <td>59.540925</td>\n",
       "      <td>North Pacific Ocean</td>\n",
       "      <td>C9435</td>\n",
       "      <td>5595.065102</td>\n",
       "      <td>isobath</td>\n",
       "      <td>2000</td>\n",
       "      <td>-2000</td>\n",
       "      <td>13899.304480</td>\n",
       "      <td>2002-05-10 03:56:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>205</td>\n",
       "      <td>5</td>\n",
       "      <td>5/10/02 21:23</td>\n",
       "      <td>36.412600</td>\n",
       "      <td>121.578203</td>\n",
       "      <td>106</td>\n",
       "      <td>C</td>\n",
       "      <td>112</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>205.637501</td>\n",
       "      <td>59.540925</td>\n",
       "      <td>North Pacific Ocean</td>\n",
       "      <td>C9435</td>\n",
       "      <td>1575.927777</td>\n",
       "      <td>isobath</td>\n",
       "      <td>2000</td>\n",
       "      <td>-2000</td>\n",
       "      <td>8257.446358</td>\n",
       "      <td>2002-05-10 21:23:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>205</td>\n",
       "      <td>7</td>\n",
       "      <td>5/11/02 0:24</td>\n",
       "      <td>36.427500</td>\n",
       "      <td>121.544404</td>\n",
       "      <td>86</td>\n",
       "      <td>C</td>\n",
       "      <td>115</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>205.637501</td>\n",
       "      <td>59.540925</td>\n",
       "      <td>North Pacific Ocean</td>\n",
       "      <td>C9435</td>\n",
       "      <td>5199.343990</td>\n",
       "      <td>isobath</td>\n",
       "      <td>2000</td>\n",
       "      <td>-2000</td>\n",
       "      <td>13913.299500</td>\n",
       "      <td>2002-05-11 00:24:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>205</td>\n",
       "      <td>8</td>\n",
       "      <td>5/11/02 1:47</td>\n",
       "      <td>36.445400</td>\n",
       "      <td>121.585703</td>\n",
       "      <td>303</td>\n",
       "      <td>C</td>\n",
       "      <td>116</td>\n",
       "      <td>237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>205.637501</td>\n",
       "      <td>59.540925</td>\n",
       "      <td>North Pacific Ocean</td>\n",
       "      <td>C9435</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>isobath</td>\n",
       "      <td>2000</td>\n",
       "      <td>-2000</td>\n",
       "      <td>9145.574006</td>\n",
       "      <td>2002-05-11 01:47:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1575</th>\n",
       "      <td>1505</td>\n",
       "      <td>164</td>\n",
       "      <td>6/13/15 4:27</td>\n",
       "      <td>37.167600</td>\n",
       "      <td>123.083799</td>\n",
       "      <td>868</td>\n",
       "      <td>C</td>\n",
       "      <td>135</td>\n",
       "      <td>0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>...</td>\n",
       "      <td>164.004633</td>\n",
       "      <td>29.545458</td>\n",
       "      <td>North Pacific Ocean</td>\n",
       "      <td>C7472</td>\n",
       "      <td>4273.853688</td>\n",
       "      <td>isobath</td>\n",
       "      <td>2000</td>\n",
       "      <td>-2000</td>\n",
       "      <td>8668.120247</td>\n",
       "      <td>2015-06-13 04:27:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1576</th>\n",
       "      <td>1505</td>\n",
       "      <td>165</td>\n",
       "      <td>6/13/15 21:39</td>\n",
       "      <td>37.535801</td>\n",
       "      <td>123.307900</td>\n",
       "      <td>1518</td>\n",
       "      <td>C</td>\n",
       "      <td>162</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>...</td>\n",
       "      <td>45.164910</td>\n",
       "      <td>14.393829</td>\n",
       "      <td>North Pacific Ocean</td>\n",
       "      <td>C7535</td>\n",
       "      <td>57.179111</td>\n",
       "      <td>isobath</td>\n",
       "      <td>2000</td>\n",
       "      <td>-2000</td>\n",
       "      <td>1145.054797</td>\n",
       "      <td>2015-06-13 21:39:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1577</th>\n",
       "      <td>1505</td>\n",
       "      <td>166</td>\n",
       "      <td>6/13/15 23:24</td>\n",
       "      <td>37.531101</td>\n",
       "      <td>123.190098</td>\n",
       "      <td>85</td>\n",
       "      <td>C</td>\n",
       "      <td>160</td>\n",
       "      <td>62100</td>\n",
       "      <td>3726.0</td>\n",
       "      <td>...</td>\n",
       "      <td>126.730235</td>\n",
       "      <td>39.861287</td>\n",
       "      <td>North Pacific Ocean</td>\n",
       "      <td>C7519</td>\n",
       "      <td>10968.390420</td>\n",
       "      <td>isobath</td>\n",
       "      <td>2000</td>\n",
       "      <td>-2000</td>\n",
       "      <td>17378.109900</td>\n",
       "      <td>2015-06-13 23:24:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578</th>\n",
       "      <td>1505</td>\n",
       "      <td>167</td>\n",
       "      <td>6/14/15 1:59</td>\n",
       "      <td>37.448701</td>\n",
       "      <td>123.085400</td>\n",
       "      <td>70</td>\n",
       "      <td>C</td>\n",
       "      <td>156</td>\n",
       "      <td>4479</td>\n",
       "      <td>55031.0</td>\n",
       "      <td>...</td>\n",
       "      <td>126.730235</td>\n",
       "      <td>39.861287</td>\n",
       "      <td>North Pacific Ocean</td>\n",
       "      <td>C7519</td>\n",
       "      <td>4975.714651</td>\n",
       "      <td>isobath</td>\n",
       "      <td>2000</td>\n",
       "      <td>-2000</td>\n",
       "      <td>15995.253420</td>\n",
       "      <td>2015-06-14 01:59:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1579</th>\n",
       "      <td>1505</td>\n",
       "      <td>168</td>\n",
       "      <td>6/14/15 3:33</td>\n",
       "      <td>37.401201</td>\n",
       "      <td>123.121797</td>\n",
       "      <td>1340</td>\n",
       "      <td>C</td>\n",
       "      <td>154</td>\n",
       "      <td>39</td>\n",
       "      <td>1605.0</td>\n",
       "      <td>...</td>\n",
       "      <td>126.730235</td>\n",
       "      <td>39.861287</td>\n",
       "      <td>North Pacific Ocean</td>\n",
       "      <td>C7519</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>isobath</td>\n",
       "      <td>2000</td>\n",
       "      <td>-2000</td>\n",
       "      <td>6779.528895</td>\n",
       "      <td>2015-06-14 03:33:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1580 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRUISE  HAUL_NO    NET_IN_TIME  NET_IN_LAT  NET_IN_LON  BOTTOM_DEPTH  \\\n",
       "0        205        2   5/10/02 2:47   36.463799  121.528496           172   \n",
       "1        205        3   5/10/02 3:56   36.423999  121.544102            86   \n",
       "2        205        5  5/10/02 21:23   36.412600  121.578203           106   \n",
       "3        205        7   5/11/02 0:24   36.427500  121.544404            86   \n",
       "4        205        8   5/11/02 1:47   36.445400  121.585703           303   \n",
       "...      ...      ...            ...         ...         ...           ...   \n",
       "1575    1505      164   6/13/15 4:27   37.167600  123.083799           868   \n",
       "1576    1505      165  6/13/15 21:39   37.535801  123.307900          1518   \n",
       "1577    1505      166  6/13/15 23:24   37.531101  123.190098            85   \n",
       "1578    1505      167   6/14/15 1:59   37.448701  123.085400            70   \n",
       "1579    1505      168   6/14/15 3:33   37.401201  123.121797          1340   \n",
       "\n",
       "     JRES_STRATA  STATION  T..SPINIFERA  E..PACIFICA  ...      Length  \\\n",
       "0              C      114          1965          0.0  ...  205.637501   \n",
       "1              C      115             4          0.0  ...  205.637501   \n",
       "2              C      112             0          0.0  ...  205.637501   \n",
       "3              C      115             4          0.0  ...  205.637501   \n",
       "4              C      116           237          0.0  ...  205.637501   \n",
       "...          ...      ...           ...          ...  ...         ...   \n",
       "1575           C      135             0         39.0  ...  164.004633   \n",
       "1576           C      162             0        132.0  ...   45.164910   \n",
       "1577           C      160         62100       3726.0  ...  126.730235   \n",
       "1578           C      156          4479      55031.0  ...  126.730235   \n",
       "1579           C      154            39       1605.0  ...  126.730235   \n",
       "\n",
       "          Width                Ocean  Canyon_ID  Distance_to_Canyon    LTYPE  \\\n",
       "0     59.540925  North Pacific Ocean      C9435            0.000000  isobath   \n",
       "1     59.540925  North Pacific Ocean      C9435         5595.065102  isobath   \n",
       "2     59.540925  North Pacific Ocean      C9435         1575.927777  isobath   \n",
       "3     59.540925  North Pacific Ocean      C9435         5199.343990  isobath   \n",
       "4     59.540925  North Pacific Ocean      C9435            0.000000  isobath   \n",
       "...         ...                  ...        ...                 ...      ...   \n",
       "1575  29.545458  North Pacific Ocean      C7472         4273.853688  isobath   \n",
       "1576  14.393829  North Pacific Ocean      C7535           57.179111  isobath   \n",
       "1577  39.861287  North Pacific Ocean      C7519        10968.390420  isobath   \n",
       "1578  39.861287  North Pacific Ocean      C7519         4975.714651  isobath   \n",
       "1579  39.861287  North Pacific Ocean      C7519            0.000000  isobath   \n",
       "\n",
       "     INCREM  DEPTH  Distance_to_200_Isobath                time  \n",
       "0      2000  -2000             17917.300130 2002-05-10 02:47:00  \n",
       "1      2000  -2000             13899.304480 2002-05-10 03:56:00  \n",
       "2      2000  -2000              8257.446358 2002-05-10 21:23:00  \n",
       "3      2000  -2000             13913.299500 2002-05-11 00:24:00  \n",
       "4      2000  -2000              9145.574006 2002-05-11 01:47:00  \n",
       "...     ...    ...                      ...                 ...  \n",
       "1575   2000  -2000              8668.120247 2015-06-13 04:27:00  \n",
       "1576   2000  -2000              1145.054797 2015-06-13 21:39:00  \n",
       "1577   2000  -2000             17378.109900 2015-06-13 23:24:00  \n",
       "1578   2000  -2000             15995.253420 2015-06-14 01:59:00  \n",
       "1579   2000  -2000              6779.528895 2015-06-14 03:33:00  \n",
       "\n",
       "[1580 rows x 34 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read in csv file in to panda dataframe\n",
    "ds_krill = pd.read_csv(krill_file)\n",
    "#calculate time in np.datetime64\n",
    "tem_time=np.ones(len(ds_krill),dtype='datetime64[ns]')\n",
    "for i in range(len(ds_krill)):\n",
    "    tstr = parse_time(ds_krill.NET_IN_TIME[i])\n",
    "    tem_time[i]=np.datetime64(tstr)\n",
    "ds_krill['time']=tem_time\n",
    "#scale lat/lon\n",
    "ds_krill['NET_IN_LAT']=ds_krill.NET_IN_LAT/100.\n",
    "ds_krill['NET_IN_LON']=ds_krill.NET_IN_LON/100.\n",
    "#print data\n",
    "ds_krill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start a cluster, a group of computers that will work together.\n",
    "\n",
    "(A cluster is the key to big data analysis on on Cloud.)\n",
    "\n",
    "- This will set up a [dask kubernetes](https://docs.dask.org/en/latest/setup/kubernetes.html) cluster for your analysis and give you a path that you can paste into the top of the Dask dashboard to visualize parts of your cluster.  \n",
    "- You don't need to paste the link below into the Dask dashboard for this to work, but it will help you visualize progress.\n",
    "- Try 20 workers to start (during the tutorial) but you can increase to speed things up later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_gateway import Gateway\n",
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gateway = Gateway()\n",
    "cluster = gateway.new_cluster()\n",
    "cluster.adapt(minimum=1, maximum=20)\n",
    "client = Client(cluster)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** ☝️ Don’t forget to click the link above or copy it to the Dask dashboard on the left to view the scheduler dashboard! **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = get_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "##create the data\n",
    "#sla_monthly = ds_aviso['sla'].resample(time='1MS').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sst_climatology = sst_timeseries.groupby('time.dayofyear').mean()\n",
    "#sst_anomaly = sst_timeseries.groupby('time.dayofyear')-sst_climatology\n",
    "#sst_anomaly_monthly = sst_anomaly.resample(time='1MS').mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in data:\n",
    "    ds_data=data[name]\n",
    "    print('name',name)\n",
    "    for var in ds_data:\n",
    "        var_tem=ds_data[var].attrs['var_name']\n",
    "        ds_krill[var_tem]=np.ones(len(ds_krill))*np.NaN\n",
    "        ds_krill[var_tem].attrs=ds_data[var].attrs\n",
    "    print('var',var_tem)\n",
    "    for i in range(len(ds_krill)):\n",
    "        if ds_krill.time[i]<ds_data.time.min():\n",
    "            continue\n",
    "        if ds_krill.time[i]>ds_data.time.max():\n",
    "            continue\n",
    "        t1,t2 = ds_krill.time[i]-np.timedelta64(24,'h'), ds_krill.time[i]+np.timedelta64(24,'h')\n",
    "        lat1,lat2=ds_krill.Lat[i]-.5,ds_krill.Lat[i]+.5\n",
    "        lon1,lon2=ds_krill.Lon[i]-.5,ds_krill.Lon[i]+.5\n",
    "        tem = ds_data.sel(time=slice(t1,t2),lat=slice(lat1,lat2),lon=slice(lon1,lon2)).load()\n",
    "        tem = tem.interp(time=ds_krill.time[i],lat=ds_krill.Lat[i],lon=ds_krill.Lon[i])\n",
    "        #tem = tem.load()\n",
    "        for var in ds_data:\n",
    "            var_tem=ds_data[var].attrs['var_name']\n",
    "            ds_krill[var_tem][i]=tem[var].data\n",
    "        if int(i/100)*100==i:\n",
    "            print(i,len(ds_krill))\n",
    "#at topo info\n",
    "#interp will create a new 2D array, to avoid that put the lat/lon into dataarrays\n",
    "ds_topo=data['topo']\n",
    "new_lat = xr.DataArray(ds_krill.Lat.values, dims='new_dim')\n",
    "new_lon = xr.DataArray(ds_krill.Lon.values, dims='new_dim')\n",
    "ds_krill['ETOPO_depth'] = ds_topo.z.interp(lat=new_lat, lon=new_lon,method='nearest')\n",
    "\n",
    "#output data\n",
    "ds_krill.to_csv(filename_bird_out)\n",
    "ds_krill = xr.Dataset.from_dataframe(ds_krill)\n",
    "ds_krill.to_netcdf(filename_bird_out_netcdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in clim:\n",
    "    ds_data=clim[name]\n",
    "    print('name',name)\n",
    "    for var in ds_data:\n",
    "        var_tem=ds_data[var].attrs['var_name']+'_clim'\n",
    "        ds_bird[var_tem]=np.ones(len(ds_bird))*np.NaN\n",
    "        ds_bird[var_tem].attrs=ds_data[var].attrs\n",
    "    print('var',var_tem)\n",
    "    for i in range(len(ds_bird)):\n",
    "        t1,t2 = ds_bird.time[i]-np.timedelta64(24,'h'), ds_bird.time[i]+np.timedelta64(24,'h')\n",
    "        lat1,lat2=ds_bird.Lat[i]-.5,ds_bird.Lat[i]+.5\n",
    "        lon1,lon2=ds_bird.Lon[i]-.5,ds_bird.Lon[i]+.5\n",
    "        tem = ds_data.sel(dayofyear=ds_bird.time[i].dayofyear,lat=slice(lat1,lat2),lon=slice(lon1,lon2)).load()\n",
    "        tem = tem.interp(lat=ds_bird.Lat[i],lon=ds_bird.Lon[i])\n",
    "        for var in ds_data:\n",
    "            var_tem=ds_data[var].attrs['var_name']+'_clim'\n",
    "            ds_bird[var_tem][i]=tem[var].data\n",
    "            \n",
    "#output data\n",
    "ds_bird.to_csv(filename_bird_out_final)\n",
    "DS_bird = xr.Dataset.from_dataframe(ds_bird)\n",
    "DS_bird.to_netcdf(filename_bird_out_netcdf_final)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
